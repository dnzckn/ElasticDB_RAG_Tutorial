{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setting up Elastic DB and Kibana\n",
    "\n",
    "Kibana is optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow quick start guide using Docker for Elastic DB: https://www.elastic.co/guide/en/elasticsearch/reference/7.17/getting-started.html\n",
    "\n",
    "Also run a terminal to get to kibana from the gui download the sample global flight dataset\n",
    "\n",
    "test in another terminal to see if your db is working\n",
    "bash: curl -X GET http://localhost:9200/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and run Elasticsearch\n",
    "\n",
    "Install and start Docker Desktop.\n",
    "Run:\n",
    "\n",
    "```python\n",
    "docker network create elastic\n",
    "docker pull docker.elastic.co/elasticsearch/elasticsearch:7.17.25\n",
    "docker run --name es01-test --net elastic -p 127.0.0.1:9200:9200 -p 127.0.0.1:9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.17.25\n",
    "```\n",
    "Install and run Kibana\n",
    "\n",
    "To analyze, visualize, and manage Elasticsearch data using an intuitive UI, install Kibana.\n",
    "\n",
    "In a new terminal session, run:\n",
    "\n",
    "```python\n",
    "docker pull docker.elastic.co/kibana/kibana:7.17.25\n",
    "docker run --name kib01-test --net elastic -p 127.0.0.1:5601:5601 -e \"ELASTICSEARCH_HOSTS=http://es01-test:9200\" docker.elastic.co/kibana/kibana:7.17.25\n",
    "To access Kibana, go to http://localhost:5601\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooking up your API Key (takes about 50Â¢ to add 1536 dimension vector embeddings to 13000 entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"replace_with_your_api_key\"\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(f\"API Key loaded: {openai_api_key is not None}\")\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Set the embedding model\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\" # this embedding is 1536 dimensional e.g. [0.1, 0.1, 0.3] is 3 dimensional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '574b453b4115', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'PK-zP1pNQwehaNMvWa62VQ', 'version': {'number': '7.17.25', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': 'f9b6b57d1d0f76e2d14291c04fb50abeb642cfbf', 'build_date': '2024-10-16T22:06:36.904732810Z', 'build_snapshot': False, 'lucene_version': '8.11.3', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_70983/1369099557.py:12: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  print(es.info())\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "import openai\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch.helpers import reindex\n",
    "import time\n",
    "import tiktoken # for truncating long inputs, not necessary if you can tailor inputs ahead of embedding\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\", basic_auth=(\"elastic\"))\n",
    "print(es.info())\n",
    "source_index = \"kibana_sample_data_flights\"\n",
    "target_index = \"flights_with_embeddings\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuild = False\n",
    "if rebuild: # run the following if you want to delete your database\n",
    "    es.indices.delete(index=target_index, ignore=[404])\n",
    "    print(f\"Deleted the {target_index} index.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=EMBEDDING_MODEL):\n",
    "    \"\"\"for embedding your query\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=model\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "def get_embeddings(texts, model=EMBEDDING_MODEL):\n",
    "    \"\"\"for adding an embedding vector to each entry in database\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=texts,\n",
    "        model=model\n",
    "    )\n",
    "    return [data.embedding for data in response.data]\n",
    "\n",
    "def prepare_text_for_embedding(doc_source, max_tokens=8191, model=EMBEDDING_MODEL, truncate=True):\n",
    "    # Extract specified fields\n",
    "    fields_to_include = [\n",
    "        ('Flight Number', 'FlightNum'),\n",
    "        ('Carrier', 'Carrier'),\n",
    "        ('From', 'OriginCityName'),\n",
    "        ('Origin Country', 'OriginCountry'),\n",
    "        ('To', 'DestCityName'),\n",
    "        ('Destination Country', 'DestCountry'),\n",
    "        ('Distance Kilometers', 'DistanceKilometers'),\n",
    "        ('Distance Miles', 'DistanceMiles'),\n",
    "        ('Flight Time Hour', 'FlightTimeHour'),\n",
    "        ('Weather at Origin', 'OriginWeather'),\n",
    "        ('Weather at Destination', 'DestWeather'),\n",
    "        ('Average Ticket Price', 'AvgTicketPrice'),\n",
    "        ('Delay', 'FlightDelay'),\n",
    "        ('Cancellation', 'Cancelled'),\n",
    "        ('Date', 'timestamp')\n",
    "    ]\n",
    "    \n",
    "    # Build the text parts\n",
    "    text_parts = []\n",
    "    for label, field in fields_to_include:\n",
    "        value = doc_source.get(field, 'N/A')\n",
    "        if isinstance(value, bool):\n",
    "            value = 'Yes' if value else 'No'\n",
    "        else:\n",
    "            value = str(value)\n",
    "        text_parts.append(f\"{label}: {value}\")\n",
    "    \n",
    "    # Include additional fields\n",
    "    additional_fields = set(doc_source.keys()) - {field for _, field in fields_to_include}\n",
    "    for field in additional_fields:\n",
    "        value = doc_source.get(field, '')\n",
    "        if isinstance(value, (dict, list)):\n",
    "            value = json.dumps(value)\n",
    "        else:\n",
    "            value = str(value)\n",
    "        text_parts.append(f\"{field}: {value}\")\n",
    "    \n",
    "    combined_text = '\\n'.join(text_parts)\n",
    "    \n",
    "    if truncate:\n",
    "        # Use tiktoken to count tokens and truncate if necessary\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        tokens = encoding.encode(combined_text)\n",
    "        \n",
    "        if len(tokens) > max_tokens:\n",
    "            # Truncate tokens to max_tokens\n",
    "            tokens = tokens[:max_tokens]\n",
    "            # Decode tokens back to text\n",
    "            combined_text = encoding.decode(tokens)\n",
    "    \n",
    "    return combined_text\n",
    "\n",
    "def fetch_documents_in_batches(index, es_client, batch_size=100):\n",
    "    query = {\"query\": {\"match_all\": {}}}\n",
    "    scan_response = helpers.scan(es_client, index=index, query=query, scroll='1m')\n",
    "    batch = []\n",
    "    for doc in scan_response:\n",
    "        batch.append(doc)\n",
    "        if len(batch) >= batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "\n",
    "def create_or_update_index(es_client, index_name):\n",
    "    mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"embedding\": {\n",
    "                    \"type\": \"dense_vector\",\n",
    "                    \"dims\": 1536  # Dimensions of the embedding vector\n",
    "                },\n",
    "                # Include other fields as needed\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    if not es_client.indices.exists(index=index_name):\n",
    "        es_client.indices.create(index=index_name, body=mapping)\n",
    "    else:\n",
    "        # Update the existing index mapping if necessary\n",
    "        es_client.indices.put_mapping(index=index_name, body=mapping[\"mappings\"])\n",
    "\n",
    "def load_progress(progress_file):\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            processed_ids = set(json.load(f))\n",
    "    else:\n",
    "        processed_ids = set()\n",
    "    return processed_ids\n",
    "\n",
    "def save_progress(progress_file, processed_ids):\n",
    "    with open(progress_file, 'w') as f:\n",
    "        json.dump(list(processed_ids), f)\n",
    "\n",
    "def process_documents(es_client, source_index, target_index, batch_size=100, progress_file='progress.json'):\n",
    "    create_or_update_index(es_client, target_index)\n",
    "    processed_ids = load_progress(progress_file)\n",
    "    total_docs = es_client.count(index=source_index)['count']\n",
    "    \n",
    "    # Initialize progress bar\n",
    "    pbar = tqdm(total=total_docs, desc=\"Processing documents\", initial=len(processed_ids))\n",
    "    \n",
    "    for batch in fetch_documents_in_batches(source_index, es_client, batch_size):\n",
    "        actions = []\n",
    "        texts = []\n",
    "        doc_ids = []\n",
    "        doc_sources = []\n",
    "        for doc in batch:\n",
    "            doc_id = doc['_id']\n",
    "            if doc_id in processed_ids:\n",
    "                pbar.update(1)\n",
    "                continue  # Skip already processed documents\n",
    "            doc_source = doc['_source']\n",
    "            combined_text = prepare_text_for_embedding(doc_source)\n",
    "            if combined_text.strip():\n",
    "                texts.append(combined_text)\n",
    "                doc_ids.append(doc_id)\n",
    "                doc_sources.append(doc_source)  # Keep the original source to include other fields\n",
    "            else:\n",
    "                # Handle empty text\n",
    "                doc_source['embedding'] = [0] * 1536\n",
    "                action = {\n",
    "                    \"_index\": target_index,\n",
    "                    \"_id\": doc_id,\n",
    "                    \"_source\": doc_source\n",
    "                }\n",
    "                actions.append(action)\n",
    "                processed_ids.add(doc_id)\n",
    "                pbar.update(1)\n",
    "        \n",
    "        if texts:\n",
    "            try:\n",
    "                # Generate embeddings in batch\n",
    "                embeddings = get_embeddings(texts)\n",
    "                \n",
    "                # Prepare actions for bulk indexing\n",
    "                for doc_id, embedding, doc_source in zip(doc_ids, embeddings, doc_sources):\n",
    "                    doc_source['embedding'] = embedding\n",
    "                    action = {\n",
    "                        \"_index\": target_index,\n",
    "                        \"_id\": doc_id,\n",
    "                        \"_source\": doc_source\n",
    "                    }\n",
    "                    actions.append(action)\n",
    "                    processed_ids.add(doc_id)\n",
    "                    pbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError during embedding generation: {e}\")\n",
    "                # Save progress before exiting\n",
    "                save_progress(progress_file, processed_ids)\n",
    "                pbar.close()\n",
    "                return\n",
    "        \n",
    "        # Bulk index the documents\n",
    "        if actions:\n",
    "            try:\n",
    "                helpers.bulk(es_client, actions)\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError during bulk indexing: {e}\")\n",
    "                # Save progress before exiting\n",
    "                save_progress(progress_file, processed_ids)\n",
    "                pbar.close()\n",
    "                return\n",
    "        \n",
    "        # Save progress after each batch\n",
    "        save_progress(progress_file, processed_ids)\n",
    "    \n",
    "    pbar.close()\n",
    "    print(\"Processing completed.\")\n",
    "    \n",
    "    \n",
    "def search_index(es_client, index_name, query_text, top_k=5):\n",
    "    # Generate embedding for the query text\n",
    "    query_embedding = get_embedding(query_text)\n",
    "\n",
    "    # Build the search query using script_score and cosineSimilarity\n",
    "    search_query = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"match_all\": {}},\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n",
    "                    \"params\": {\n",
    "                        \"query_vector\": query_embedding\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Perform the search\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "\n",
    "    # Display the results\n",
    "    for hit in response['hits']['hits']:\n",
    "        score = hit['_score']\n",
    "        source = hit['_source']\n",
    "        print(f\"Score: {score}\")\n",
    "        print(f\"Flight Number: {source.get('FlightNum', 'N/A')}\")\n",
    "        print(f\"Carrier: {source.get('Carrier', 'N/A')}\")\n",
    "        print(f\"From: {source.get('OriginCityName', 'N/A')} ({source.get('OriginCountry', 'N/A')})\")\n",
    "        print(f\"To: {source.get('DestCityName', 'N/A')} ({source.get('DestCountry', 'N/A')})\")\n",
    "        print(f\"Distance: {source.get('DistanceKilometers', 0):,.2f} km \"\n",
    "              f\"({source.get('DistanceMiles', 0):,.2f} miles)\")\n",
    "        flight_time_hour = source.get('FlightTimeHour', 0)\n",
    "        print(f\"Flight Time: {float(flight_time_hour):.2f} hours\" if flight_time_hour else \"Flight Time: N/A\")\n",
    "        print(f\"Weather at Origin: {source.get('OriginWeather', 'N/A')}\")\n",
    "        print(f\"Weather at Destination: {source.get('DestWeather', 'N/A')}\")\n",
    "        print(f\"Average Ticket Price: ${source.get('AvgTicketPrice', 0):,.2f}\")\n",
    "        print(f\"Delay: {'Yes' if source.get('FlightDelay', False) else 'No'}\")\n",
    "        print(f\"Cancellation: {'Yes' if source.get('Cancelled', False) else 'No'}\")\n",
    "        print(f\"Date: {source.get('timestamp', 'N/A')}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    return response  # Return response for use in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_70983/310303618.py:100: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  if not es_client.indices.exists(index=index_name):\n",
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_70983/310303618.py:104: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es_client.indices.put_mapping(index=index_name, body=mapping[\"mappings\"])\n",
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_70983/310303618.py:121: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  total_docs = es_client.count(index=source_index)['count']\n",
      "Processing documents: 100%|ââââââââââ| 13059/13059 [00:00<?, ?it/s]/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_70983/310303618.py:79: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  for doc in scan_response:\n",
      "Processing documents: 26118it [00:03, 3910.71it/s]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# takes about 2-3 minutes\n",
    "process_documents(es, source_index, target_index, batch_size=50, progress_file='progress.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"show me a cheap flight to mexico from canada\"\n",
    "response = search_index(es, target_index, query_text, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add ons: if you want to enahnce your response with a more sophisticated model you can serve the response to GPT 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant information from the top search results\n",
    "def extract_context_from_hits(hits):\n",
    "    context = \"\"\n",
    "    for hit in hits:\n",
    "        source = hit['_source']\n",
    "        flight_info = (\n",
    "            f\"Flight Number: {source.get('FlightNum', 'N/A')}\\n\"\n",
    "            f\"Carrier: {source.get('Carrier', 'N/A')}\\n\"\n",
    "            f\"From: {source.get('OriginCityName', 'N/A')} ({source.get('OriginCountry', 'N/A')})\\n\"\n",
    "            f\"To: {source.get('DestCityName', 'N/A')} ({source.get('DestCountry', 'N/A')})\\n\"\n",
    "            f\"Distance: {source.get('DistanceKilometers', 0):,.2f} km \"\n",
    "            f\"({source.get('DistanceMiles', 0):,.2f} miles)\\n\"\n",
    "            f\"Flight Time: {float(source.get('FlightTimeHour', 0)):.2f} hours\\n\"\n",
    "            f\"Weather at Origin: {source.get('OriginWeather', 'N/A')}\\n\"\n",
    "            f\"Weather at Destination: {source.get('DestWeather', 'N/A')}\\n\"\n",
    "            f\"Average Ticket Price: ${source.get('AvgTicketPrice', 0):,.2f}\\n\"\n",
    "            f\"Delay: {'Yes' if source.get('FlightDelay', False) else 'No'}\\n\"\n",
    "            f\"Cancellation: {'Yes' if source.get('Cancelled', False) else 'No'}\\n\"\n",
    "            f\"Date: {source.get('timestamp', 'N/A')}\\n\"\n",
    "            \"----------------------------------------\\n\"\n",
    "        )\n",
    "        context += flight_info\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add on a more sophisticated LLM to make the response more pleasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract context from search results\n",
    "context = extract_context_from_hits(response['hits']['hits'])\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "# Use OpenAI's ChatCompletion API\n",
    "def generate_response(question, context):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides flight information.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {question}\\n\\nContext:\\n{context}\\n\\nAnswer the question using the context provided. If the answer is not in the context, say 'I could not find the information in the provided data.'\"}\n",
    "    ]\n",
    "\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages)\n",
    "\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "# Generate the response\n",
    "answer = generate_response(query_text, context)\n",
    "\n",
    "print(\"Assistant's Response:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on how to potentially integrate with Kibana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview\n",
    "To integrate your custom search functionality into Kibana, we'll explore the following approaches:\n",
    "\n",
    "Create a Custom Kibana Plugin: Develop a plugin that adds new routes and UI components to Kibana.\n",
    "Use Kibana Vega Visualizations: Utilize Vega or Vega-Lite visualizations to create custom visualizations that can interact with Elasticsearch.\n",
    "Embed External Applications: If necessary, build an external web application that interfaces with Elasticsearch and OpenAI, and embed it within Kibana using iframes or Canvas.\n",
    "For your use case, developing a custom Kibana plugin is the most direct way to integrate your Python code into Kibana's UI. Below, I'll provide detailed steps on how to create a Kibana plugin that can:\n",
    "\n",
    "Accept user queries.\n",
    "Perform vector searches using Elasticsearch.\n",
    "Display results within Kibana.\n",
    "Optionally, integrate with OpenAI's API to generate additional responses.\n",
    "Prerequisites\n",
    "Knowledge of JavaScript/TypeScript: Kibana plugins are developed using JavaScript or TypeScript and React.\n",
    "Development Environment: Set up a development environment for Kibana plugin development.\n",
    "Elasticsearch and Kibana Version Compatibility: Ensure that your Elasticsearch and Kibana versions are compatible.\n",
    "Step-by-Step Guide\n",
    "Step 1: Set Up Your Development Environment\n",
    "1.1 Clone the Kibana Repository\n",
    "Clone the Kibana repository from GitHub to get access to the necessary development tools and plugin generator.\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "git clone https://github.com/elastic/kibana.git\n",
    "cd kibana\n",
    "1.2 Check Out the Correct Branch\n",
    "Switch to the branch that matches your Kibana version. For example, if you're using Kibana 8.5.0:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "git checkout v8.5.0\n",
    "1.3 Install Dependencies\n",
    "Install the required dependencies using Yarn (Kibana uses Yarn for package management):\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "yarn kbn bootstrap\n",
    "This command sets up the development environment and installs all necessary packages.\n",
    "\n",
    "Step 2: Generate a New Plugin\n",
    "Kibana provides a plugin generator to scaffold a new plugin.\n",
    "\n",
    "2.1 Run the Plugin Generator\n",
    "bash\n",
    "Copy code\n",
    "node scripts/generate_plugin.js\n",
    "2.2 Provide Plugin Details\n",
    "You'll be prompted to provide details about your plugin:\n",
    "\n",
    "Plugin Name: e.g., vector_search_plugin\n",
    "Plugin ID: e.g., vectorSearchPlugin\n",
    "Description: e.g., A plugin to perform vector searches and display results\n",
    "Owner Name: Your name or your organization's name\n",
    "Client-side: Yes (since we'll be building UI components)\n",
    "Server-side: Yes (if you need to add server routes)\n",
    "Step 3: Develop the Plugin\n",
    "Now that you have the plugin scaffolded, you can start developing it.\n",
    "\n",
    "3.1 Understand the Plugin Structure\n",
    "Your plugin directory will be located at kibana/plugins/vector_search_plugin/. It will have the following structure:\n",
    "\n",
    "public/: Contains client-side code (UI components).\n",
    "server/: Contains server-side code (routes, handlers).\n",
    "kibana.json: Plugin manifest file.\n",
    "3.2 Implement the Client-Side UI\n",
    "In the public/ directory, you'll develop the React components that make up your plugin's UI.\n",
    "\n",
    "3.2.1 Create a Query Input Component\n",
    "In public/components/, create a new component called QueryInput.tsx.\n",
    "\n",
    "tsx\n",
    "Copy code\n",
    "import React, { useState } from 'react';\n",
    "import { EuiFieldText, EuiButton } from '@elastic/eui';\n",
    "\n",
    "interface QueryInputProps {\n",
    "  onSearch: (queryText: string) => void;\n",
    "}\n",
    "\n",
    "export const QueryInput: React.FC<QueryInputProps> = ({ onSearch }) => {\n",
    "  const [queryText, setQueryText] = useState('');\n",
    "\n",
    "  const handleSearch = () => {\n",
    "    onSearch(queryText);\n",
    "  };\n",
    "\n",
    "  return (\n",
    "    <div>\n",
    "      <EuiFieldText\n",
    "        placeholder=\"Enter your query...\"\n",
    "        value={queryText}\n",
    "        onChange={(e) => setQueryText(e.target.value)}\n",
    "      />\n",
    "      <EuiButton onClick={handleSearch}>Search</EuiButton>\n",
    "    </div>\n",
    "  );\n",
    "};\n",
    "This component provides an input field and a search button.\n",
    "\n",
    "3.2.2 Display Search Results\n",
    "Create a SearchResults.tsx component to display the results.\n",
    "\n",
    "tsx\n",
    "Copy code\n",
    "import React from 'react';\n",
    "\n",
    "interface SearchResult {\n",
    "  score: number;\n",
    "  source: any;\n",
    "}\n",
    "\n",
    "interface SearchResultsProps {\n",
    "  results: SearchResult[];\n",
    "}\n",
    "\n",
    "export const SearchResults: React.FC<SearchResultsProps> = ({ results }) => {\n",
    "  return (\n",
    "    <div>\n",
    "      {results.map((hit, index) => {\n",
    "        const source = hit.source;\n",
    "        return (\n",
    "          <div key={index}>\n",
    "            <h3>Score: {hit.score}</h3>\n",
    "            <p>Flight Number: {source.FlightNum}</p>\n",
    "            <p>Carrier: {source.Carrier}</p>\n",
    "            <p>\n",
    "              From: {source.OriginCityName} ({source.OriginCountry})\n",
    "            </p>\n",
    "            <p>\n",
    "              To: {source.DestCityName} ({source.DestCountry})\n",
    "            </p>\n",
    "            {/* Add more fields as needed */}\n",
    "            <hr />\n",
    "          </div>\n",
    "        );\n",
    "      })}\n",
    "    </div>\n",
    "  );\n",
    "};\n",
    "3.2.3 Combine Components in the Main App\n",
    "In public/application.tsx, import your components and handle the logic.\n",
    "\n",
    "tsx\n",
    "Copy code\n",
    "import React, { useState } from 'react';\n",
    "import { CoreStart } from '@kbn/core/public';\n",
    "import { QueryInput } from './components/QueryInput';\n",
    "import { SearchResults } from './components/SearchResults';\n",
    "\n",
    "interface AppProps {\n",
    "  http: CoreStart['http'];\n",
    "}\n",
    "\n",
    "export const App: React.FC<AppProps> = ({ http }) => {\n",
    "  const [results, setResults] = useState([]);\n",
    "\n",
    "  const handleSearch = async (queryText: string) => {\n",
    "    try {\n",
    "      const response = await http.post('/api/vector_search_plugin/search', {\n",
    "        body: JSON.stringify({ queryText }),\n",
    "      });\n",
    "      setResults(response.hits);\n",
    "    } catch (error) {\n",
    "      console.error('Error performing search:', error);\n",
    "    }\n",
    "  };\n",
    "\n",
    "  return (\n",
    "    <div>\n",
    "      <QueryInput onSearch={handleSearch} />\n",
    "      <SearchResults results={results} />\n",
    "    </div>\n",
    "  );\n",
    "};\n",
    "3.3 Implement Server-Side Route\n",
    "In order to perform the search, you need to define a server-side route that your UI can call.\n",
    "\n",
    "3.3.1 Define the Route\n",
    "In server/routes/, create search.ts.\n",
    "\n",
    "ts\n",
    "Copy code\n",
    "import { IRouter } from '@kbn/core/server';\n",
    "import { schema } from '@kbn/config-schema';\n",
    "\n",
    "export function defineRoutes(router: IRouter) {\n",
    "  router.post(\n",
    "    {\n",
    "      path: '/api/vector_search_plugin/search',\n",
    "      validate: {\n",
    "        body: schema.object({\n",
    "          queryText: schema.string(),\n",
    "        }),\n",
    "      },\n",
    "    },\n",
    "    async (context, request, response) => {\n",
    "      const { queryText } = request.body;\n",
    "\n",
    "      // Perform vector search using Elasticsearch client\n",
    "      const esClient = (await context.core).elasticsearch.client.asCurrentUser;\n",
    "\n",
    "      // Generate embedding for the query text\n",
    "      // You need to integrate your embedding generation here\n",
    "      // Since server-side code is in Node.js, you might need to use a Node.js OpenAI client\n",
    "\n",
    "      // For the purpose of this example, we'll assume you have a function `getEmbedding`\n",
    "      const queryEmbedding = await getEmbedding(queryText);\n",
    "\n",
    "      // Build the search query\n",
    "      const searchQuery = {\n",
    "        size: 5,\n",
    "        query: {\n",
    "          script_score: {\n",
    "            query: { match_all: {} },\n",
    "            script: {\n",
    "              source: \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n",
    "              params: {\n",
    "                query_vector: queryEmbedding,\n",
    "              },\n",
    "            },\n",
    "          },\n",
    "        },\n",
    "      };\n",
    "\n",
    "      // Perform the search\n",
    "      const esResponse = await esClient.search({\n",
    "        index: 'flights_with_embeddings',\n",
    "        body: searchQuery,\n",
    "      });\n",
    "\n",
    "      // Return the results\n",
    "      return response.ok({\n",
    "        body: {\n",
    "          hits: esResponse.hits.hits.map((hit: any) => ({\n",
    "            score: hit._score,\n",
    "            source: hit._source,\n",
    "          })),\n",
    "        },\n",
    "      });\n",
    "    }\n",
    "  );\n",
    "}\n",
    "3.3.2 Register the Route\n",
    "In server/plugin.ts, import and register the route:\n",
    "\n",
    "ts\n",
    "Copy code\n",
    "import { defineRoutes } from './routes/search';\n",
    "\n",
    "public setup(core: CoreSetup) {\n",
    "  const router = core.http.createRouter();\n",
    "  defineRoutes(router);\n",
    "}\n",
    "3.4 Integrate OpenAI Embedding Generation\n",
    "Since the server-side code is in Node.js, you'll need to use the OpenAI Node.js client.\n",
    "\n",
    "3.4.1 Install OpenAI Node.js SDK\n",
    "In your plugin directory, install the OpenAI SDK:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "cd plugins/vector_search_plugin\n",
    "npm install openai\n",
    "3.4.2 Implement the Embedding Function\n",
    "In server/lib/embedding.ts, create the function to generate embeddings:\n",
    "\n",
    "ts\n",
    "Copy code\n",
    "import { Configuration, OpenAIApi } from 'openai';\n",
    "\n",
    "const configuration = new Configuration({\n",
    "  apiKey: 'YOUR_OPENAI_API_KEY', // Securely manage your API key\n",
    "});\n",
    "\n",
    "const openai = new OpenAIApi(configuration);\n",
    "\n",
    "export async function getEmbedding(text: string): Promise<number[]> {\n",
    "  const response = await openai.createEmbedding({\n",
    "    model: 'text-embedding-ada-002',\n",
    "    input: text,\n",
    "  });\n",
    "\n",
    "  return response.data.data[0].embedding;\n",
    "}\n",
    "Security Note: Never hard-code your API key in the codebase. Use environment variables or Kibana's secure settings to manage sensitive information.\n",
    "\n",
    "3.4.3 Securely Manage API Keys\n",
    "Use Kibana's config or secrets management to store your OpenAI API key.\n",
    "\n",
    "In config/kibana.yml, add:\n",
    "yaml\n",
    "Copy code\n",
    "vectorSearchPlugin.openaiApiKey: YOUR_OPENAI_API_KEY\n",
    "Access the API key in your code:\n",
    "ts\n",
    "Copy code\n",
    "const config = context.config.get<{ openaiApiKey: string }>();\n",
    "const openaiApiKey = config.openaiApiKey;\n",
    "Step 4: Build and Run Kibana with Your Plugin\n",
    "4.1 Build the Plugin\n",
    "From the Kibana root directory, run:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "yarn build\n",
    "4.2 Start Kibana\n",
    "bash\n",
    "Copy code\n",
    "yarn start\n",
    "This will start Kibana with your plugin included.\n",
    "\n",
    "Step 5: Test Your Plugin\n",
    "Navigate to Kibana in your web browser.\n",
    "Find your plugin in the side navigation bar.\n",
    "Enter a query in the input field and perform a search.\n",
    "Verify that the search results are displayed as expected.\n",
    "Considerations and Limitations\n",
    "1. API Key Security\n",
    "Ensure that your OpenAI API key is securely managed and not exposed in the client-side code. Use Kibana's secure settings or environment variables.\n",
    "\n",
    "2. Rate Limits and Performance\n",
    "Be mindful of OpenAI's rate limits. Implement caching or request throttling as needed.\n",
    "\n",
    "3. Error Handling\n",
    "Add proper error handling in your server and client code to handle exceptions and provide feedback to users.\n",
    "\n",
    "4. Elasticsearch Version Compatibility\n",
    "Make sure that the Elasticsearch features you're using (e.g., script_score, cosineSimilarity) are supported in your Elasticsearch version.\n",
    "\n",
    "5. Plugin Maintenance\n",
    "Custom plugins need to be maintained and may require updates when upgrading Kibana versions.\n",
    "\n",
    "Alternative Approaches\n",
    "Option 1: Use Kibana Vega Visualizations\n",
    "Vega is a visualization grammar that allows for custom visualizations within Kibana.\n",
    "\n",
    "Pros:\n",
    "No need to develop a full plugin.\n",
    "Can perform Elasticsearch queries and visualize results.\n",
    "Cons:\n",
    "Limited in terms of custom logic and external API calls.\n",
    "Embedding OpenAI API calls within Vega might not be feasible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
