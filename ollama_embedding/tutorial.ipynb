{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setting up Elastic DB and Kibana\n",
    "\n",
    "Kibana is optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow quick start guide using Docker for Elastic DB: https://www.elastic.co/guide/en/elasticsearch/reference/7.17/getting-started.html\n",
    "\n",
    "Also run a terminal to get to kibana from the gui download the sample global flight dataset\n",
    "\n",
    "test in another terminal to see if your db is working\n",
    "bash: curl -X GET http://localhost:9200/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and run Elasticsearch\n",
    "\n",
    "Install and start Docker Desktop.\n",
    "Run:\n",
    "\n",
    "```python\n",
    "docker network create elastic\n",
    "docker pull docker.elastic.co/elasticsearch/elasticsearch:7.17.25\n",
    "docker run --name es01-test --net elastic -p 127.0.0.1:9200:9200 -p 127.0.0.1:9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.17.25\n",
    "```\n",
    "Install and run Kibana\n",
    "\n",
    "To analyze, visualize, and manage Elasticsearch data using an intuitive UI, install Kibana.\n",
    "\n",
    "In a new terminal session, run:\n",
    "\n",
    "```python\n",
    "docker pull docker.elastic.co/kibana/kibana:7.17.27\n",
    "docker run --name kib01-test --net elastic -p 127.0.0.1:5601:5601 -e \"ELASTICSEARCH_HOSTS=http://es01-test:9200\" docker.elastic.co/kibana/kibana:7.17.27\n",
    "To access Kibana, go to http://localhost:5601\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooking up your API Key (takes about 50¢ to add 1536 dimension vector embeddings to 13000 entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Ollama:\n",
    "1. https://ollama.com/download\n",
    "\n",
    "2. `ollama run llama3.2` in terminal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '4d52facd65b5', 'cluster_name': 'docker-cluster', 'cluster_uuid': '0IO0Xm81SVe13I2_Y-cAZA', 'version': {'number': '7.17.25', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': 'f9b6b57d1d0f76e2d14291c04fb50abeb642cfbf', 'build_date': '2024-10-16T22:06:36.904732810Z', 'build_snapshot': False, 'lucene_version': '8.11.3', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_39129/985251538.py:11: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  print(es.info())\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch.helpers import reindex\n",
    "import time\n",
    "import tiktoken # for truncating long inputs, not necessary if you can tailor inputs ahead of embedding\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\", basic_auth=(\"elastic\"))\n",
    "print(es.info())\n",
    "source_index = \"kibana_sample_data_flights\"\n",
    "target_index = \"flights_with_embeddings\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_39129/3006089610.py:2: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es_client.indices.delete(index=target_index, ignore=[404])\n",
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_39129/3006089610.py:2: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es_client.indices.delete(index=target_index, ignore=[404])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete the target index\n",
    "es_client.indices.delete(index=target_index, ignore=[404])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import ElasticsearchStore\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Elasticsearch + helpers\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 1. OLLAMA EMBEDDINGS CLASS\n",
    "# ------------------------------------------------------------------------\n",
    "OLLAMA_EMBEDDINGS_URL = \"http://localhost:11434/api/embeddings\"\n",
    "OLLAMA_MODEL_NAME = \"nomic-embed-text\"\n",
    "\n",
    "class OllamaEmbeddings(Embeddings):\n",
    "    \"\"\"\n",
    "    A simple LangChain Embeddings wrapper that calls Ollama's REST API.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = OLLAMA_MODEL_NAME):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"Compute embeddings for a list of texts.\"\"\"\n",
    "        return [self._embed(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"Compute embedding for a single query text.\"\"\"\n",
    "        return self._embed(text)\n",
    "\n",
    "    def _embed(self, text: str):\n",
    "        payload = {\"model\": self.model_name, \"prompt\": text}\n",
    "        response = requests.post(OLLAMA_EMBEDDINGS_URL, json=payload)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return data[\"embedding\"]  # expect list[float]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 2. ELASTICSEARCH SETUP\n",
    "# ------------------------------------------------------------------------\n",
    "# Connect to Elasticsearch\n",
    "es_client = Elasticsearch(\n",
    "    \"http://localhost:9200\",\n",
    "    basic_auth=(\"elastic\", \"changeme\")  # Update password if needed\n",
    ")\n",
    "\n",
    "# Indices\n",
    "source_index = \"kibana_sample_data_flights\"\n",
    "target_index = \"flights_with_embeddings_ollama_lc\"\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3. CREATE A LANGCHAIN VECTOR STORE\n",
    "# ------------------------------------------------------------------------\n",
    "ollama_embeddings = OllamaEmbeddings()\n",
    "es_store = ElasticsearchStore(\n",
    "    embedding=ollama_embeddings,\n",
    "    index_name=target_index,\n",
    "    es_connection=es_client\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 4. HELPER FUNCTIONS TO FETCH & CONVERT DOCS\n",
    "# ------------------------------------------------------------------------\n",
    "def fetch_documents_in_batches(index, es_client, batch_size=100):\n",
    "    \"\"\"\n",
    "    Generator that scans the 'source_index' in Elasticsearch and \n",
    "    yields documents in batches of `batch_size`.\n",
    "    \"\"\"\n",
    "    query = {\"query\": {\"match_all\": {}}}\n",
    "    scan = helpers.scan(\n",
    "        es_client,\n",
    "        index=index,\n",
    "        query=query,\n",
    "        scroll='1m'\n",
    "    )\n",
    "    batch = []\n",
    "    for doc in scan:\n",
    "        batch.append(doc)\n",
    "        if len(batch) >= batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    # yield the last batch if leftover\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "def convert_es_docs_to_lc_docs(es_docs):\n",
    "    \"\"\"\n",
    "    Convert a list of Elasticsearch docs into LangChain Document objects.\n",
    "    Decide how you want to build the page_content from the fields.\n",
    "    \"\"\"\n",
    "    lc_docs = []\n",
    "    for d in es_docs:\n",
    "        source = d['_source']\n",
    "        text = \"\\n\".join(f\"{k}: {v}\" for k, v in source.items())\n",
    "        \n",
    "        lc_docs.append(\n",
    "            Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"_id\": d[\"_id\"],\n",
    "                    \"_index\": d[\"_index\"],\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    return lc_docs\n",
    "\n",
    "# Optional splitter if docs are large\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 5. INDEX DOCUMENTS INTO THE NEW VECTOR INDEX\n",
    "# ------------------------------------------------------------------------\n",
    "def index_documents_from_source(\n",
    "    source_idx: str,\n",
    "    target_store,\n",
    "    batch_size: int = 50,\n",
    "    max_docs: int | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    1. Scan the source Elasticsearch index in batches.\n",
    "    2. Convert to LangChain Documents.\n",
    "    3. (Optional) split them.\n",
    "    4. Add them to the VectorStore (which embeds + indexes).\n",
    "    \"\"\"\n",
    "    total_docs = es_client.count(index=source_idx)['count']\n",
    "    if max_docs is not None:\n",
    "        total = min(total_docs, max_docs)\n",
    "    else:\n",
    "        total = total_docs\n",
    "\n",
    "    pbar = tqdm(total=total, desc=\"Indexing\")\n",
    "    \n",
    "    docs_embedded = 0\n",
    "    for batch in fetch_documents_in_batches(source_idx, es_client, batch_size):\n",
    "        if max_docs is not None and docs_embedded >= max_docs:\n",
    "            break\n",
    "        \n",
    "        remaining = max_docs - docs_embedded if max_docs is not None else len(batch)\n",
    "        partial_batch = batch[:remaining]\n",
    "        \n",
    "        lc_docs = convert_es_docs_to_lc_docs(partial_batch)\n",
    "        splitted_docs = splitter.split_documents(lc_docs)\n",
    "        \n",
    "        target_store.add_documents(splitted_docs)\n",
    "        \n",
    "        docs_embedded += len(partial_batch)\n",
    "        pbar.update(len(partial_batch))\n",
    "\n",
    "    pbar.close()\n",
    "    print(\n",
    "        f\"Completed indexing from '{source_idx}' to '{target_index}' \"\n",
    "        f\"with {docs_embedded} docs processed.\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 6. SCRIPT_SCORE SEARCH (Cosine Similarity) \n",
    "# ------------------------------------------------------------------------\n",
    "def script_score_search(index_name: str, query_vector: list[float], top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Perform a script_score cosine similarity query on the field 'vector'.\n",
    "    Returns the raw Elasticsearch response, excluding the 'vector' field.\n",
    "    \"\"\"\n",
    "    body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"match_all\": {}},\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vector, 'vector') + 1.0\",\n",
    "                    \"params\": {\n",
    "                        \"query_vector\": query_vector\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    # We tell Elasticsearch not to return the 'vector' field in the results\n",
    "    return es_client.search(\n",
    "        index=index_name,\n",
    "        body=body,\n",
    "        _source_excludes=[\"vector\"]  # This ensures the 'vector' field is omitted\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 7. EXTRACT FLIGHT INFO FROM HITS\n",
    "# ------------------------------------------------------------------------\n",
    "def extract_context_from_hits(hits):\n",
    "    \"\"\"\n",
    "    Format flight-related fields from each document, parsing them from the\n",
    "    '_source.text' string. Returns a multi-line string with flight info.\n",
    "    \"\"\"\n",
    "    context = \"\"\n",
    "    for hit in hits:\n",
    "        # 'text' holds lines like \"FlightNum: NF156JH\\nDestCountry: IN\\n...\"\n",
    "        text_str = hit[\"_source\"][\"text\"]\n",
    "        \n",
    "        # Parse each line into a dict, e.g. {\"FlightNum\": \"NF156JH\", \"DestCountry\": \"IN\", ...}\n",
    "        lines_dict = {}\n",
    "        for line in text_str.split(\"\\n\"):\n",
    "            if \": \" in line:  # ensure the line is key: value format\n",
    "                key, val = line.split(\": \", 1)\n",
    "                lines_dict[key] = val.strip()\n",
    "        \n",
    "        # Convert certain numeric/string fields\n",
    "        def to_float(s):\n",
    "            try:\n",
    "                return float(s)\n",
    "            except ValueError:\n",
    "                return 0.0\n",
    "\n",
    "        flight_num = lines_dict.get(\"FlightNum\", \"N/A\")\n",
    "        carrier = lines_dict.get(\"Carrier\", \"N/A\")\n",
    "        origin_city = lines_dict.get(\"OriginCityName\", \"N/A\")\n",
    "        origin_country = lines_dict.get(\"OriginCountry\", \"N/A\")\n",
    "        dest_city = lines_dict.get(\"DestCityName\", \"N/A\")\n",
    "        dest_country = lines_dict.get(\"DestCountry\", \"N/A\")\n",
    "        distance_km = to_float(lines_dict.get(\"DistanceKilometers\", \"0\"))\n",
    "        distance_mi = to_float(lines_dict.get(\"DistanceMiles\", \"0\"))\n",
    "        flight_time_hr = to_float(lines_dict.get(\"FlightTimeHour\", \"0\"))\n",
    "        origin_weather = lines_dict.get(\"OriginWeather\", \"N/A\")\n",
    "        dest_weather = lines_dict.get(\"DestWeather\", \"N/A\")\n",
    "        avg_ticket_price = to_float(lines_dict.get(\"AvgTicketPrice\", \"0\"))\n",
    "        timestamp = lines_dict.get(\"timestamp\", \"N/A\")\n",
    "        \n",
    "        # Convert booleans\n",
    "        flight_delay_str = lines_dict.get(\"FlightDelay\", \"False\")\n",
    "        flight_delay = (flight_delay_str.lower() == \"true\")\n",
    "        cancelled_str = lines_dict.get(\"Cancelled\", \"False\")\n",
    "        cancelled = (cancelled_str.lower() == \"true\")\n",
    "\n",
    "        # Format final text block\n",
    "        flight_info = (\n",
    "            f\"Flight Number: {flight_num}\\n\"\n",
    "            f\"Carrier: {carrier}\\n\"\n",
    "            f\"From: {origin_city} ({origin_country})\\n\"\n",
    "            f\"To: {dest_city} ({dest_country})\\n\"\n",
    "            f\"Distance: {distance_km:,.2f} km ({distance_mi:,.2f} miles)\\n\"\n",
    "            f\"Flight Time: {flight_time_hr:.2f} hours\\n\"\n",
    "            f\"Weather at Origin: {origin_weather}\\n\"\n",
    "            f\"Weather at Destination: {dest_weather}\\n\"\n",
    "            f\"Average Ticket Price: ${avg_ticket_price:,.2f}\\n\"\n",
    "            f\"Delay: {'Yes' if flight_delay else 'No'}\\n\"\n",
    "            f\"Cancellation: {'Yes' if cancelled else 'No'}\\n\"\n",
    "            f\"Date: {timestamp}\\n\"\n",
    "            \"----------------------------------------\\n\"\n",
    "        )\n",
    "        context += flight_info\n",
    "\n",
    "    return context\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 8. COMBINED QUERY FUNCTION\n",
    "# ------------------------------------------------------------------------\n",
    "def query_vector_store(query_text: str, top_k=5):\n",
    "    \"\"\"\n",
    "    1. Embed the query with OllamaEmbeddings.\n",
    "    2. Run a script_score search on the newly created index.\n",
    "    3. Return the hits so we can do something with them (like extract flight info).\n",
    "    \"\"\"\n",
    "    print(f\"\\nQuery: {query_text}\")\n",
    "    query_vec = ollama_embeddings.embed_query(query_text)\n",
    "\n",
    "    response = script_score_search(target_index, query_vec, top_k)\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    return hits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_12081/3279058841.py:130: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  total_docs = es_client.count(index=source_idx)['count']\n",
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_12081/3279058841.py:82: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  for doc in scan:\n",
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_12081/3279058841.py:141: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  break\n",
      "Indexing: 100%|██████████| 200/200 [01:21<00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed indexing from 'kibana_sample_data_flights' to 'flights_with_embeddings_ollama_lc' with 200 docs processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "index_documents_from_source(source_index, es_store, batch_size=50, max_docs=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: show me a cheap flight to mexico from canada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_12081/3279058841.py:183: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  return es_client.search(\n"
     ]
    }
   ],
   "source": [
    "# 2) Example query\n",
    "query_text = \"show me a cheap flight to mexico from canada\"\n",
    "hits = query_vector_store(query_text, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXTRACTED CONTEXT:\n",
      " Flight Number: FZ1FWP0\n",
      "Carrier: Kibana Airlines\n",
      "From: Mexico City (MX)\n",
      "To: Ottawa (CA)\n",
      "Distance: 3,589.92 km (2,230.67 miles)\n",
      "Flight Time: 3.52 hours\n",
      "Weather at Origin: Rain\n",
      "Weather at Destination: Clear\n",
      "Average Ticket Price: $937.73\n",
      "Delay: No\n",
      "Cancellation: No\n",
      "Date: 2025-01-13T20:42:03\n",
      "----------------------------------------\n",
      "Flight Number: NF156JH\n",
      "Carrier: Kibana Airlines\n",
      "From: Mexico City (MX)\n",
      "To: Hyderabad (IN)\n",
      "Distance: 15,939.01 km (9,904.04 miles)\n",
      "Flight Time: 18.98 hours\n",
      "Weather at Origin: Rain\n",
      "Weather at Destination: Hail\n",
      "Average Ticket Price: $655.36\n",
      "Delay: No\n",
      "Cancellation: No\n",
      "Date: 2025-01-13T05:32:47\n",
      "----------------------------------------\n",
      "Flight Number: U9MODFN\n",
      "Carrier: ES-Air\n",
      "From: Mexico City (MX)\n",
      "To: San Francisco (US)\n",
      "Distance: 3,027.21 km (1,881.02 miles)\n",
      "Flight Time: 2.66 hours\n",
      "Weather at Origin: Hail\n",
      "Weather at Destination: Heavy Fog\n",
      "Average Ticket Price: $928.53\n",
      "Delay: No\n",
      "Cancellation: No\n",
      "Date: 2025-01-13T08:31:31\n",
      "----------------------------------------\n",
      "Flight Number: S6BPNLR\n",
      "Carrier: JetBeats\n",
      "From: Mexico City (MX)\n",
      "To: Manchester (GB)\n",
      "Distance: 8,734.74 km (5,427.52 miles)\n",
      "Flight Time: 9.10 hours\n",
      "Weather at Origin: Cloudy\n",
      "Weather at Destination: Thunder & Lightning\n",
      "Average Ticket Price: $481.71\n",
      "Delay: No\n",
      "Cancellation: Yes\n",
      "Date: 2025-01-13T23:04:05\n",
      "----------------------------------------\n",
      "Flight Number: 58U013N\n",
      "Carrier: Kibana Airlines\n",
      "From: Mexico City (MX)\n",
      "To: Xi'an (CN)\n",
      "Distance: 13,358.24 km (8,300.43 miles)\n",
      "Flight Time: 13.10 hours\n",
      "Weather at Origin: Damaging Wind\n",
      "Weather at Destination: Clear\n",
      "Average Ticket Price: $730.04\n",
      "Delay: No\n",
      "Cancellation: No\n",
      "Date: 2025-01-13T05:13:00\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3) Extract context from hits\n",
    "context_str = extract_context_from_hits(hits)\n",
    "print(\"EXTRACTED CONTEXT:\\n\", context_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a reranking algorithm (cross encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_39129/2635410136.py:129: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  total_docs = es_client.count(index=source_idx)['count']\n",
      "Indexing:   0%|          | 0/200 [00:00<?, ?it/s]/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_39129/2635410136.py:85: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  for doc in scan:\n",
      "/Users/deniz/anaconda3/envs/ds310/lib/python3.10/site-packages/langchain_community/vectorstores/elasticsearch.py:917: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  if self.client.indices.exists(index=index_name):\n",
      "/Users/deniz/anaconda3/envs/ds310/lib/python3.10/site-packages/langchain_community/vectorstores/elasticsearch.py:943: ElasticsearchWarning: Parameter [similarity] has no effect on type [dense_vector] and will be removed in future\n",
      "  self.client.indices.create(index=index_name, **indexSettings)\n",
      "/Users/deniz/anaconda3/envs/ds310/lib/python3.10/site-packages/langchain_community/vectorstores/elasticsearch.py:943: ElasticsearchWarning: Parameter [index] has no effect on type [dense_vector] and will be removed in future\n",
      "  self.client.indices.create(index=index_name, **indexSettings)\n",
      "/Users/deniz/anaconda3/envs/ds310/lib/python3.10/site-packages/langchain_community/vectorstores/elasticsearch.py:943: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  self.client.indices.create(index=index_name, **indexSettings)\n",
      "/Users/deniz/anaconda3/envs/ds310/lib/python3.10/site-packages/langchain_community/vectorstores/elasticsearch.py:994: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  success, failed = bulk(\n",
      "Indexing: 100%|██████████| 200/200 [01:21<00:00,  2.41it/s]/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_39129/2635410136.py:140: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  break\n",
      "Indexing: 100%|██████████| 200/200 [01:21<00:00,  2.44it/s]\n",
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_39129/2635410136.py:181: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  return es_client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed indexing from 'kibana_sample_data_flights' to 'flights_with_embeddings_ollama_lc' with 200 docs processed.\n",
      "\n",
      "Query: show me a cheap flight to mexico from canada\n",
      "Initial vector search returned 20 hits.\n",
      "Re-ranked hits. Now taking top 5.\n",
      "FINAL RE-RANKED CONTEXT:\n",
      " Flight Number: FZ1FWP0\n",
      "Carrier: Kibana Airlines\n",
      "From: Mexico City (MX)\n",
      "To: Ottawa (CA)\n",
      "Distance: 3,589.92 km (2,230.67 miles)\n",
      "Flight Time: 3.52 hours\n",
      "Weather at Origin: Rain\n",
      "Weather at Destination: Clear\n",
      "Average Ticket Price: $937.73\n",
      "Delay: No\n",
      "Cancellation: No\n",
      "Date: 2025-01-13T20:42:03\n",
      "----------------------------------------\n",
      "Flight Number: NF156JH\n",
      "Carrier: Kibana Airlines\n",
      "From: Mexico City (MX)\n",
      "To: Hyderabad (IN)\n",
      "Distance: 15,939.01 km (9,904.04 miles)\n",
      "Flight Time: 18.98 hours\n",
      "Weather at Origin: Rain\n",
      "Weather at Destination: Hail\n",
      "Average Ticket Price: $655.36\n",
      "Delay: No\n",
      "Cancellation: No\n",
      "Date: 2025-01-13T05:32:47\n",
      "----------------------------------------\n",
      "Flight Number: S6BPNLR\n",
      "Carrier: JetBeats\n",
      "From: Mexico City (MX)\n",
      "To: Manchester (GB)\n",
      "Distance: 8,734.74 km (5,427.52 miles)\n",
      "Flight Time: 9.10 hours\n",
      "Weather at Origin: Cloudy\n",
      "Weather at Destination: Thunder & Lightning\n",
      "Average Ticket Price: $481.71\n",
      "Delay: No\n",
      "Cancellation: Yes\n",
      "Date: 2025-01-13T23:04:05\n",
      "----------------------------------------\n",
      "Flight Number: P0WMFH7\n",
      "Carrier: Logstash Airways\n",
      "From: Mexico City (MX)\n",
      "To: Shanghai (CN)\n",
      "Distance: 12,915.60 km (8,025.38 miles)\n",
      "Flight Time: 11.33 hours\n",
      "Weather at Origin: Heavy Fog\n",
      "Weather at Destination: Clear\n",
      "Average Ticket Price: $922.50\n",
      "Delay: No\n",
      "Cancellation: Yes\n",
      "Date: 2025-01-13T02:13:46\n",
      "----------------------------------------\n",
      "Flight Number: U9MODFN\n",
      "Carrier: ES-Air\n",
      "From: Mexico City (MX)\n",
      "To: San Francisco (US)\n",
      "Distance: 3,027.21 km (1,881.02 miles)\n",
      "Flight Time: 2.66 hours\n",
      "Weather at Origin: Hail\n",
      "Weather at Destination: Heavy Fog\n",
      "Average Ticket Price: $928.53\n",
      "Delay: No\n",
      "Cancellation: No\n",
      "Date: 2025-01-13T08:31:31\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "DATAFRAME VIEW:\n",
      "  FlightNum           Carrier   OriginCity OriginCountry       DestCity  \\\n",
      "0   FZ1FWP0   Kibana Airlines  Mexico City            MX         Ottawa   \n",
      "1   NF156JH   Kibana Airlines  Mexico City            MX      Hyderabad   \n",
      "2   S6BPNLR          JetBeats  Mexico City            MX     Manchester   \n",
      "3   P0WMFH7  Logstash Airways  Mexico City            MX       Shanghai   \n",
      "4   U9MODFN            ES-Air  Mexico City            MX  San Francisco   \n",
      "\n",
      "  DestCountry  DistanceKilometers      DistanceMiles      FlightTimeHour  \\\n",
      "0          CA   3589.917091471716  2230.671063160962   3.519526560266388   \n",
      "1          IN  15939.010935856824  9904.042228297258   18.97501301887717   \n",
      "2          GB   8734.740207760542  5427.515936779546   9.098687716417231   \n",
      "3          CN  12915.599427519877  8025.381414737853   11.32947318203498   \n",
      "4          US    3027.20705171588  1881.019254873961  2.6554447822069123   \n",
      "\n",
      "  OriginWeather          DestWeather     AvgTicketPrice FlightDelay Cancelled  \\\n",
      "0          Rain                Clear  937.7339299906702       False     False   \n",
      "1          Rain                 Hail  655.3579734059554       False     False   \n",
      "2        Cloudy  Thunder & Lightning  481.7131078562487       False      True   \n",
      "3     Heavy Fog                Clear   922.499077027416       False      True   \n",
      "4          Hail            Heavy Fog  928.5267648528594       False     False   \n",
      "\n",
      "                  Date     Score  ReRankScore  \n",
      "0  2025-01-13T20:42:03  1.637937    -4.813090  \n",
      "1  2025-01-13T05:32:47  1.599879    -7.584251  \n",
      "2  2025-01-13T23:04:05  1.589916    -7.681205  \n",
      "3  2025-01-13T02:13:46  1.565417    -7.815392  \n",
      "4  2025-01-13T08:31:31  1.591420    -7.995126  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import pandas as pd  # <-- Make sure you have pandas installed\n",
    "\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import ElasticsearchStore\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Elasticsearch + helpers\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "# For cross-encoder re-ranking\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 1. OLLAMA EMBEDDINGS CLASS\n",
    "# ------------------------------------------------------------------------\n",
    "OLLAMA_EMBEDDINGS_URL = \"http://localhost:11434/api/embeddings\"\n",
    "OLLAMA_MODEL_NAME = \"nomic-embed-text\"\n",
    "\n",
    "class OllamaEmbeddings(Embeddings):\n",
    "    \"\"\"\n",
    "    A simple LangChain Embeddings wrapper that calls Ollama's REST API.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = OLLAMA_MODEL_NAME):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"Compute embeddings for a list of texts using Ollama.\"\"\"\n",
    "        return [self._embed(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"Compute embedding for a single query text.\"\"\"\n",
    "        return self._embed(text)\n",
    "\n",
    "    def _embed(self, text: str):\n",
    "        payload = {\"model\": self.model_name, \"prompt\": text}\n",
    "        response = requests.post(OLLAMA_EMBEDDINGS_URL, json=payload)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return data[\"embedding\"]  # a list[float]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 2. ELASTICSEARCH SETUP\n",
    "# ------------------------------------------------------------------------\n",
    "es_client = Elasticsearch(\n",
    "    \"http://localhost:9200\",\n",
    "    basic_auth=(\"elastic\", \"changeme\")  # Update your ES credentials as needed\n",
    ")\n",
    "\n",
    "source_index = \"kibana_sample_data_flights\"\n",
    "target_index = \"flights_with_embeddings_ollama_lc\"\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3. CREATE A LANGCHAIN VECTOR STORE\n",
    "# ------------------------------------------------------------------------\n",
    "ollama_embeddings = OllamaEmbeddings()\n",
    "es_store = ElasticsearchStore(\n",
    "    embedding=ollama_embeddings,\n",
    "    index_name=target_index,\n",
    "    es_connection=es_client\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 4. HELPER FUNCTIONS TO FETCH & CONVERT DOCS\n",
    "# ------------------------------------------------------------------------\n",
    "def fetch_documents_in_batches(index, es_client, batch_size=100):\n",
    "    \"\"\"\n",
    "    Generator that scans the 'source_index' in Elasticsearch and \n",
    "    yields documents in batches of `batch_size`.\n",
    "    \"\"\"\n",
    "    query = {\"query\": {\"match_all\": {}}}\n",
    "    scan = helpers.scan(\n",
    "        es_client,\n",
    "        index=index,\n",
    "        query=query,\n",
    "        scroll='1m'\n",
    "    )\n",
    "    batch = []\n",
    "    for doc in scan:\n",
    "        batch.append(doc)\n",
    "        if len(batch) >= batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "def convert_es_docs_to_lc_docs(es_docs):\n",
    "    \"\"\"\n",
    "    Convert a list of Elasticsearch docs into LangChain Document objects.\n",
    "    Decide how you want to build the page_content from the fields.\n",
    "    \"\"\"\n",
    "    lc_docs = []\n",
    "    for d in es_docs:\n",
    "        source = d['_source']\n",
    "        text = \"\\n\".join(f\"{k}: {v}\" for k, v in source.items())\n",
    "        \n",
    "        lc_docs.append(\n",
    "            Document(\n",
    "                page_content=text,\n",
    "                metadata={\"_id\": d[\"_id\"], \"_index\": d[\"_index\"]}\n",
    "            )\n",
    "        )\n",
    "    return lc_docs\n",
    "\n",
    "# Optional splitter if docs are large\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 5. INDEX DOCUMENTS INTO THE NEW VECTOR INDEX\n",
    "# ------------------------------------------------------------------------\n",
    "def index_documents_from_source(\n",
    "    source_idx: str,\n",
    "    target_store,\n",
    "    batch_size: int = 50,\n",
    "    max_docs: int | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    1. Scan the source Elasticsearch index in batches.\n",
    "    2. Convert to LangChain Documents.\n",
    "    3. (Optional) split them.\n",
    "    4. Add them to the VectorStore (which embeds + indexes).\n",
    "    \"\"\"\n",
    "    total_docs = es_client.count(index=source_idx)['count']\n",
    "    if max_docs is not None:\n",
    "        total = min(total_docs, max_docs)\n",
    "    else:\n",
    "        total = total_docs\n",
    "\n",
    "    pbar = tqdm(total=total, desc=\"Indexing\")\n",
    "    \n",
    "    docs_embedded = 0\n",
    "    for batch in fetch_documents_in_batches(source_idx, es_client, batch_size):\n",
    "        if max_docs is not None and docs_embedded >= max_docs:\n",
    "            break\n",
    "        \n",
    "        remaining = max_docs - docs_embedded if max_docs is not None else len(batch)\n",
    "        partial_batch = batch[:remaining]\n",
    "        \n",
    "        lc_docs = convert_es_docs_to_lc_docs(partial_batch)\n",
    "        splitted_docs = splitter.split_documents(lc_docs)\n",
    "        \n",
    "        target_store.add_documents(splitted_docs)\n",
    "        \n",
    "        docs_embedded += len(partial_batch)\n",
    "        pbar.update(len(partial_batch))\n",
    "\n",
    "    pbar.close()\n",
    "    print(\n",
    "        f\"Completed indexing from '{source_idx}' to '{target_index}' \"\n",
    "        f\"with {docs_embedded} docs processed.\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 6. SCRIPT_SCORE SEARCH (Cosine Similarity)\n",
    "# ------------------------------------------------------------------------\n",
    "def script_score_search(index_name: str, query_vector: list[float], top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Perform a script_score cosine similarity query on the field 'vector'.\n",
    "    Returns the raw ES response, excluding the 'vector' field.\n",
    "    \"\"\"\n",
    "    body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"match_all\": {}},\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vector, 'vector') + 1.0\",\n",
    "                    \"params\": {\n",
    "                        \"query_vector\": query_vector\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return es_client.search(\n",
    "        index=index_name,\n",
    "        body=body,\n",
    "        _source_excludes=[\"vector\"]  # omit the vector field from results\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 7. EXTRACT FLIGHT INFO FROM HITS (STRING)\n",
    "# ------------------------------------------------------------------------\n",
    "def extract_context_from_hits(hits):\n",
    "    \"\"\"\n",
    "    Format flight-related fields from each document,\n",
    "    parsing them from '_source.text' and returning a multi-line string.\n",
    "    \"\"\"\n",
    "    context = \"\"\n",
    "    for hit in hits:\n",
    "        text_str = hit[\"_source\"][\"text\"]\n",
    "        \n",
    "        lines_dict = {}\n",
    "        for line in text_str.split(\"\\n\"):\n",
    "            if \": \" in line:\n",
    "                key, val = line.split(\": \", 1)\n",
    "                lines_dict[key] = val.strip()\n",
    "        \n",
    "        def to_float(s):\n",
    "            try:\n",
    "                return float(s)\n",
    "            except ValueError:\n",
    "                return 0.0\n",
    "\n",
    "        flight_num = lines_dict.get(\"FlightNum\", \"N/A\")\n",
    "        carrier = lines_dict.get(\"Carrier\", \"N/A\")\n",
    "        origin_city = lines_dict.get(\"OriginCityName\", \"N/A\")\n",
    "        origin_country = lines_dict.get(\"OriginCountry\", \"N/A\")\n",
    "        dest_city = lines_dict.get(\"DestCityName\", \"N/A\")\n",
    "        dest_country = lines_dict.get(\"DestCountry\", \"N/A\")\n",
    "        distance_km = to_float(lines_dict.get(\"DistanceKilometers\", \"0\"))\n",
    "        distance_mi = to_float(lines_dict.get(\"DistanceMiles\", \"0\"))\n",
    "        flight_time_hr = to_float(lines_dict.get(\"FlightTimeHour\", \"0\"))\n",
    "        origin_weather = lines_dict.get(\"OriginWeather\", \"N/A\")\n",
    "        dest_weather = lines_dict.get(\"DestWeather\", \"N/A\")\n",
    "        avg_ticket_price = to_float(lines_dict.get(\"AvgTicketPrice\", \"0\"))\n",
    "        timestamp = lines_dict.get(\"timestamp\", \"N/A\")\n",
    "        \n",
    "        flight_delay_str = lines_dict.get(\"FlightDelay\", \"False\")\n",
    "        flight_delay = (flight_delay_str.lower() == \"true\")\n",
    "        cancelled_str = lines_dict.get(\"Cancelled\", \"False\")\n",
    "        cancelled = (cancelled_str.lower() == \"true\")\n",
    "\n",
    "        flight_info = (\n",
    "            f\"Flight Number: {flight_num}\\n\"\n",
    "            f\"Carrier: {carrier}\\n\"\n",
    "            f\"From: {origin_city} ({origin_country})\\n\"\n",
    "            f\"To: {dest_city} ({dest_country})\\n\"\n",
    "            f\"Distance: {distance_km:,.2f} km ({distance_mi:,.2f} miles)\\n\"\n",
    "            f\"Flight Time: {flight_time_hr:.2f} hours\\n\"\n",
    "            f\"Weather at Origin: {origin_weather}\\n\"\n",
    "            f\"Weather at Destination: {dest_weather}\\n\"\n",
    "            f\"Average Ticket Price: ${avg_ticket_price:,.2f}\\n\"\n",
    "            f\"Delay: {'Yes' if flight_delay else 'No'}\\n\"\n",
    "            f\"Cancellation: {'Yes' if cancelled else 'No'}\\n\"\n",
    "            f\"Date: {timestamp}\\n\"\n",
    "            \"----------------------------------------\\n\"\n",
    "        )\n",
    "        context += flight_info\n",
    "    return context\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 8A. RERANK WITH CROSS-ENCODER\n",
    "# ------------------------------------------------------------------------\n",
    "def rerank_with_cross_encoder(query: str, hits: list, model_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Re-rank 'hits' using a local or hosted cross-encoder from SentenceTransformers.\n",
    "    \n",
    "    :param query: The user's query string.\n",
    "    :param hits: A list of ES hits (dicts) from the initial vector search.\n",
    "    :param model_path: The cross-encoder model name or local path.\n",
    "    :return: The same hits list with 'rerank_score' and sorted descending by that score.\n",
    "    \"\"\"\n",
    "    # Load from HF:\n",
    "    model = CrossEncoder(model_path, max_length=512)\n",
    "    # or offline:\n",
    "    # model = CrossEncoder(\"/path/to/local/ms-marco-MiniLM-L-6-v2\", max_length=512)\n",
    "    \n",
    "    pairs = []\n",
    "    for hit in hits:\n",
    "        doc_text = hit[\"_source\"][\"text\"]\n",
    "        pairs.append((query, doc_text))  # (query, doc)\n",
    "    \n",
    "    scores = model.predict(pairs)\n",
    "    \n",
    "    for hit, score in zip(hits, scores):\n",
    "        hit[\"rerank_score\"] = float(score)\n",
    "    \n",
    "    # Sort hits by rerank_score descending\n",
    "    hits.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "    return hits\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 8B. OPTIONAL: CONVERT HITS TO A PANDAS TABLE\n",
    "# ------------------------------------------------------------------------\n",
    "def hits_to_dataframe(hits):\n",
    "    \"\"\"\n",
    "    Convert the top hits into a pandas DataFrame for easier viewing.\n",
    "    We'll parse each document's '_source.text' into fields.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for hit in hits:\n",
    "        text_str = hit[\"_source\"][\"text\"]\n",
    "        \n",
    "        lines_dict = {}\n",
    "        for line in text_str.split(\"\\n\"):\n",
    "            if \": \" in line:\n",
    "                key, val = line.split(\": \", 1)\n",
    "                lines_dict[key.strip()] = val.strip()\n",
    "        \n",
    "        # Build a dict of relevant fields\n",
    "        row_data = {\n",
    "            \"FlightNum\": lines_dict.get(\"FlightNum\", \"N/A\"),\n",
    "            \"Carrier\": lines_dict.get(\"Carrier\", \"N/A\"),\n",
    "            \"OriginCity\": lines_dict.get(\"OriginCityName\", \"N/A\"),\n",
    "            \"OriginCountry\": lines_dict.get(\"OriginCountry\", \"N/A\"),\n",
    "            \"DestCity\": lines_dict.get(\"DestCityName\", \"N/A\"),\n",
    "            \"DestCountry\": lines_dict.get(\"DestCountry\", \"N/A\"),\n",
    "            \"DistanceKilometers\": lines_dict.get(\"DistanceKilometers\", \"0\"),\n",
    "            \"DistanceMiles\": lines_dict.get(\"DistanceMiles\", \"0\"),\n",
    "            \"FlightTimeHour\": lines_dict.get(\"FlightTimeHour\", \"0\"),\n",
    "            \"OriginWeather\": lines_dict.get(\"OriginWeather\", \"N/A\"),\n",
    "            \"DestWeather\": lines_dict.get(\"DestWeather\", \"N/A\"),\n",
    "            \"AvgTicketPrice\": lines_dict.get(\"AvgTicketPrice\", \"0\"),\n",
    "            \"FlightDelay\": lines_dict.get(\"FlightDelay\", \"False\"),\n",
    "            \"Cancelled\": lines_dict.get(\"Cancelled\", \"False\"),\n",
    "            \"Date\": lines_dict.get(\"timestamp\", \"N/A\"),\n",
    "            \"Score\": hit[\"_score\"],  # original vector score\n",
    "            \"ReRankScore\": hit.get(\"rerank_score\", None),  # if re-rank was applied\n",
    "        }\n",
    "        \n",
    "        rows.append(row_data)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 9. COMBINED QUERY FUNCTION\n",
    "# ------------------------------------------------------------------------\n",
    "def query_vector_store(query_text: str, top_k=20, cross_encoder_model: str=None, final_top=5):\n",
    "    \"\"\"\n",
    "    1. Embed the query with OllamaEmbeddings.\n",
    "    2. Run a script_score search on the new index (top_k docs).\n",
    "    3. If cross_encoder_model is given, re-rank the hits with a cross-encoder.\n",
    "    4. Return the top final_top hits.\n",
    "    \"\"\"\n",
    "    print(f\"\\nQuery: {query_text}\")\n",
    "    query_vec = ollama_embeddings.embed_query(query_text)\n",
    "    \n",
    "    response = script_score_search(target_index, query_vec, top_k)\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    print(f\"Initial vector search returned {len(hits)} hits.\")\n",
    "    \n",
    "    if cross_encoder_model:\n",
    "        # Re-rank\n",
    "        hits = rerank_with_cross_encoder(query_text, hits, cross_encoder_model)\n",
    "        print(f\"Re-ranked hits. Now taking top {final_top}.\")\n",
    "        hits = hits[:final_top]\n",
    "    \n",
    "    return hits\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 10. MAIN SCRIPT\n",
    "# ------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Index docs from source -> new vector index\n",
    "    index_documents_from_source(source_index, es_store, batch_size=50, max_docs=200)\n",
    "\n",
    "    # 2) Example query\n",
    "    user_query = \"show me a cheap flight to mexico from canada\"\n",
    "\n",
    "    # We retrieve top 20 via vector search, then re-rank top 20 with cross-encoder\n",
    "    # Then keep only final top 5.\n",
    "    \n",
    "    # Option A: Using Hugging Face model name (requires internet)\n",
    "    CROSS_ENCODER_MODEL_PATH = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "    \n",
    "    # Option B: For offline usage, comment out above and point to local path:\n",
    "    # CROSS_ENCODER_MODEL_PATH = \"/path/to/local/ms-marco-MiniLM-L-6-v2\"\n",
    "    \n",
    "    hits = query_vector_store(\n",
    "        user_query,\n",
    "        top_k=20,\n",
    "        cross_encoder_model=CROSS_ENCODER_MODEL_PATH,\n",
    "        final_top=5\n",
    "    )\n",
    "\n",
    "    # 3) Show a string-based summary:\n",
    "    context_str = extract_context_from_hits(hits)\n",
    "    print(\"FINAL RE-RANKED CONTEXT:\\n\", context_str)\n",
    "\n",
    "    # 4) Convert hits to a DataFrame for easy viewing:\n",
    "    df = hits_to_dataframe(hits)\n",
    "    print(\"\\nDATAFRAME VIEW:\")\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>Carrier</th>\n",
       "      <th>OriginCity</th>\n",
       "      <th>OriginCountry</th>\n",
       "      <th>DestCity</th>\n",
       "      <th>DestCountry</th>\n",
       "      <th>DistanceKilometers</th>\n",
       "      <th>DistanceMiles</th>\n",
       "      <th>FlightTimeHour</th>\n",
       "      <th>OriginWeather</th>\n",
       "      <th>DestWeather</th>\n",
       "      <th>AvgTicketPrice</th>\n",
       "      <th>FlightDelay</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>Date</th>\n",
       "      <th>Score</th>\n",
       "      <th>ReRankScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FZ1FWP0</td>\n",
       "      <td>Kibana Airlines</td>\n",
       "      <td>Mexico City</td>\n",
       "      <td>MX</td>\n",
       "      <td>Ottawa</td>\n",
       "      <td>CA</td>\n",
       "      <td>3589.917091471716</td>\n",
       "      <td>2230.671063160962</td>\n",
       "      <td>3.519526560266388</td>\n",
       "      <td>Rain</td>\n",
       "      <td>Clear</td>\n",
       "      <td>937.7339299906702</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-01-13T20:42:03</td>\n",
       "      <td>1.637937</td>\n",
       "      <td>-4.813090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NF156JH</td>\n",
       "      <td>Kibana Airlines</td>\n",
       "      <td>Mexico City</td>\n",
       "      <td>MX</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>IN</td>\n",
       "      <td>15939.010935856824</td>\n",
       "      <td>9904.042228297258</td>\n",
       "      <td>18.97501301887717</td>\n",
       "      <td>Rain</td>\n",
       "      <td>Hail</td>\n",
       "      <td>655.3579734059554</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-01-13T05:32:47</td>\n",
       "      <td>1.599879</td>\n",
       "      <td>-7.584251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S6BPNLR</td>\n",
       "      <td>JetBeats</td>\n",
       "      <td>Mexico City</td>\n",
       "      <td>MX</td>\n",
       "      <td>Manchester</td>\n",
       "      <td>GB</td>\n",
       "      <td>8734.740207760542</td>\n",
       "      <td>5427.515936779546</td>\n",
       "      <td>9.098687716417231</td>\n",
       "      <td>Cloudy</td>\n",
       "      <td>Thunder &amp; Lightning</td>\n",
       "      <td>481.7131078562487</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-01-13T23:04:05</td>\n",
       "      <td>1.589916</td>\n",
       "      <td>-7.681205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P0WMFH7</td>\n",
       "      <td>Logstash Airways</td>\n",
       "      <td>Mexico City</td>\n",
       "      <td>MX</td>\n",
       "      <td>Shanghai</td>\n",
       "      <td>CN</td>\n",
       "      <td>12915.599427519877</td>\n",
       "      <td>8025.381414737853</td>\n",
       "      <td>11.32947318203498</td>\n",
       "      <td>Heavy Fog</td>\n",
       "      <td>Clear</td>\n",
       "      <td>922.499077027416</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2025-01-13T02:13:46</td>\n",
       "      <td>1.565417</td>\n",
       "      <td>-7.815392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U9MODFN</td>\n",
       "      <td>ES-Air</td>\n",
       "      <td>Mexico City</td>\n",
       "      <td>MX</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>US</td>\n",
       "      <td>3027.20705171588</td>\n",
       "      <td>1881.019254873961</td>\n",
       "      <td>2.6554447822069123</td>\n",
       "      <td>Hail</td>\n",
       "      <td>Heavy Fog</td>\n",
       "      <td>928.5267648528594</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2025-01-13T08:31:31</td>\n",
       "      <td>1.591420</td>\n",
       "      <td>-7.995126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FlightNum           Carrier   OriginCity OriginCountry       DestCity  \\\n",
       "0   FZ1FWP0   Kibana Airlines  Mexico City            MX         Ottawa   \n",
       "1   NF156JH   Kibana Airlines  Mexico City            MX      Hyderabad   \n",
       "2   S6BPNLR          JetBeats  Mexico City            MX     Manchester   \n",
       "3   P0WMFH7  Logstash Airways  Mexico City            MX       Shanghai   \n",
       "4   U9MODFN            ES-Air  Mexico City            MX  San Francisco   \n",
       "\n",
       "  DestCountry  DistanceKilometers      DistanceMiles      FlightTimeHour  \\\n",
       "0          CA   3589.917091471716  2230.671063160962   3.519526560266388   \n",
       "1          IN  15939.010935856824  9904.042228297258   18.97501301887717   \n",
       "2          GB   8734.740207760542  5427.515936779546   9.098687716417231   \n",
       "3          CN  12915.599427519877  8025.381414737853   11.32947318203498   \n",
       "4          US    3027.20705171588  1881.019254873961  2.6554447822069123   \n",
       "\n",
       "  OriginWeather          DestWeather     AvgTicketPrice FlightDelay Cancelled  \\\n",
       "0          Rain                Clear  937.7339299906702       False     False   \n",
       "1          Rain                 Hail  655.3579734059554       False     False   \n",
       "2        Cloudy  Thunder & Lightning  481.7131078562487       False      True   \n",
       "3     Heavy Fog                Clear   922.499077027416       False      True   \n",
       "4          Hail            Heavy Fog  928.5267648528594       False     False   \n",
       "\n",
       "                  Date     Score  ReRankScore  \n",
       "0  2025-01-13T20:42:03  1.637937    -4.813090  \n",
       "1  2025-01-13T05:32:47  1.599879    -7.584251  \n",
       "2  2025-01-13T23:04:05  1.589916    -7.681205  \n",
       "3  2025-01-13T02:13:46  1.565417    -7.815392  \n",
       "4  2025-01-13T08:31:31  1.591420    -7.995126  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
