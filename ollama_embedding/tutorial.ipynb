{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setting up Elastic DB and Kibana\n",
    "\n",
    "Kibana is optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow quick start guide using Docker for Elastic DB: https://www.elastic.co/guide/en/elasticsearch/reference/7.17/getting-started.html\n",
    "\n",
    "Also run a terminal to get to kibana from the gui download the sample global flight dataset\n",
    "\n",
    "test in another terminal to see if your db is working\n",
    "bash: curl -X GET http://localhost:9200/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and run Elasticsearch\n",
    "\n",
    "Install and start Docker Desktop.\n",
    "Run:\n",
    "\n",
    "```python\n",
    "docker network create elastic\n",
    "docker pull docker.elastic.co/elasticsearch/elasticsearch:7.17.25\n",
    "docker run --name es01-test --net elastic -p 127.0.0.1:9200:9200 -p 127.0.0.1:9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.17.25\n",
    "```\n",
    "Install and run Kibana\n",
    "\n",
    "To analyze, visualize, and manage Elasticsearch data using an intuitive UI, install Kibana.\n",
    "\n",
    "In a new terminal session, run:\n",
    "\n",
    "```python\n",
    "docker pull docker.elastic.co/kibana/kibana:7.17.27\n",
    "docker run --name kib01-test --net elastic -p 127.0.0.1:5601:5601 -e \"ELASTICSEARCH_HOSTS=http://es01-test:9200\" docker.elastic.co/kibana/kibana:7.17.27\n",
    "To access Kibana, go to http://localhost:5601\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooking up your API Key (takes about 50¢ to add 1536 dimension vector embeddings to 13000 entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Ollama:\n",
    "1. https://ollama.com/download\n",
    "\n",
    "2. `ollama run llama3.2` in terminal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '4d52facd65b5', 'cluster_name': 'docker-cluster', 'cluster_uuid': '0IO0Xm81SVe13I2_Y-cAZA', 'version': {'number': '7.17.25', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': 'f9b6b57d1d0f76e2d14291c04fb50abeb642cfbf', 'build_date': '2024-10-16T22:06:36.904732810Z', 'build_snapshot': False, 'lucene_version': '8.11.3', 'minimum_wire_compatibility_version': '6.8.0', 'minimum_index_compatibility_version': '6.0.0-beta1'}, 'tagline': 'You Know, for Search'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_12081/985251538.py:11: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  print(es.info())\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch.helpers import reindex\n",
    "import time\n",
    "import tiktoken # for truncating long inputs, not necessary if you can tailor inputs ahead of embedding\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\", basic_auth=(\"elastic\"))\n",
    "print(es.info())\n",
    "source_index = \"kibana_sample_data_flights\"\n",
    "target_index = \"flights_with_embeddings\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_12081/3006089610.py:2: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  es_client.indices.delete(index=target_index, ignore=[404])\n",
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_12081/3006089610.py:2: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  es_client.indices.delete(index=target_index, ignore=[404])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'error': {'root_cause': [{'type': 'index_not_found_exception', 'reason': 'no such index [flights_with_embeddings_ollama_lc]', 'resource.type': 'index_or_alias', 'resource.id': 'flights_with_embeddings_ollama_lc', 'index_uuid': '_na_', 'index': 'flights_with_embeddings_ollama_lc'}], 'type': 'index_not_found_exception', 'reason': 'no such index [flights_with_embeddings_ollama_lc]', 'resource.type': 'index_or_alias', 'resource.id': 'flights_with_embeddings_ollama_lc', 'index_uuid': '_na_', 'index': 'flights_with_embeddings_ollama_lc'}, 'status': 404})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete the target index\n",
    "es_client.indices.delete(index=target_index, ignore=[404])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import ElasticsearchStore\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Elasticsearch + helpers\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 1. OLLAMA EMBEDDINGS CLASS\n",
    "# ------------------------------------------------------------------------\n",
    "OLLAMA_EMBEDDINGS_URL = \"http://localhost:11434/api/embeddings\"\n",
    "OLLAMA_MODEL_NAME = \"nomic-embed-text\"\n",
    "\n",
    "class OllamaEmbeddings(Embeddings):\n",
    "    \"\"\"\n",
    "    A simple LangChain Embeddings wrapper that calls Ollama's REST API.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = OLLAMA_MODEL_NAME):\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"Compute embeddings for a list of texts.\"\"\"\n",
    "        return [self._embed(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        \"\"\"Compute embedding for a single query text.\"\"\"\n",
    "        return self._embed(text)\n",
    "\n",
    "    def _embed(self, text: str):\n",
    "        payload = {\"model\": self.model_name, \"prompt\": text}\n",
    "        response = requests.post(OLLAMA_EMBEDDINGS_URL, json=payload)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        return data[\"embedding\"]  # expect list[float]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 2. ELASTICSEARCH SETUP\n",
    "# ------------------------------------------------------------------------\n",
    "# Connect to Elasticsearch\n",
    "es_client = Elasticsearch(\n",
    "    \"http://localhost:9200\",\n",
    "    basic_auth=(\"elastic\", \"changeme\")  # Update password if needed\n",
    ")\n",
    "\n",
    "# Indices\n",
    "source_index = \"kibana_sample_data_flights\"\n",
    "target_index = \"flights_with_embeddings_ollama_lc\"\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 3. CREATE A LANGCHAIN VECTOR STORE\n",
    "# ------------------------------------------------------------------------\n",
    "ollama_embeddings = OllamaEmbeddings()\n",
    "es_store = ElasticsearchStore(\n",
    "    embedding=ollama_embeddings,\n",
    "    index_name=target_index,\n",
    "    es_connection=es_client\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 4. HELPER FUNCTIONS TO FETCH & CONVERT DOCS\n",
    "# ------------------------------------------------------------------------\n",
    "def fetch_documents_in_batches(index, es_client, batch_size=100):\n",
    "    \"\"\"\n",
    "    Generator that scans the 'source_index' in Elasticsearch and \n",
    "    yields documents in batches of `batch_size`.\n",
    "    \"\"\"\n",
    "    query = {\"query\": {\"match_all\": {}}}\n",
    "    scan = helpers.scan(\n",
    "        es_client,\n",
    "        index=index,\n",
    "        query=query,\n",
    "        scroll='1m'\n",
    "    )\n",
    "    batch = []\n",
    "    for doc in scan:\n",
    "        batch.append(doc)\n",
    "        if len(batch) >= batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    # yield the last batch if leftover\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "def convert_es_docs_to_lc_docs(es_docs):\n",
    "    \"\"\"\n",
    "    Convert a list of Elasticsearch docs into LangChain Document objects.\n",
    "    Decide how you want to build the page_content from the fields.\n",
    "    \"\"\"\n",
    "    lc_docs = []\n",
    "    for d in es_docs:\n",
    "        source = d['_source']\n",
    "        text = \"\\n\".join(f\"{k}: {v}\" for k, v in source.items())\n",
    "        \n",
    "        lc_docs.append(\n",
    "            Document(\n",
    "                page_content=text,\n",
    "                metadata={\n",
    "                    \"_id\": d[\"_id\"],\n",
    "                    \"_index\": d[\"_index\"],\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    return lc_docs\n",
    "\n",
    "# Optional splitter if docs are large\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 5. INDEX DOCUMENTS INTO THE NEW VECTOR INDEX\n",
    "# ------------------------------------------------------------------------\n",
    "def index_documents_from_source(\n",
    "    source_idx: str,\n",
    "    target_store,\n",
    "    batch_size: int = 50,\n",
    "    max_docs: int | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    1. Scan the source Elasticsearch index in batches.\n",
    "    2. Convert to LangChain Documents.\n",
    "    3. (Optional) split them.\n",
    "    4. Add them to the VectorStore (which embeds + indexes).\n",
    "    \"\"\"\n",
    "    total_docs = es_client.count(index=source_idx)['count']\n",
    "    if max_docs is not None:\n",
    "        total = min(total_docs, max_docs)\n",
    "    else:\n",
    "        total = total_docs\n",
    "\n",
    "    pbar = tqdm(total=total, desc=\"Indexing\")\n",
    "    \n",
    "    docs_embedded = 0\n",
    "    for batch in fetch_documents_in_batches(source_idx, es_client, batch_size):\n",
    "        if max_docs is not None and docs_embedded >= max_docs:\n",
    "            break\n",
    "        \n",
    "        remaining = max_docs - docs_embedded if max_docs is not None else len(batch)\n",
    "        partial_batch = batch[:remaining]\n",
    "        \n",
    "        lc_docs = convert_es_docs_to_lc_docs(partial_batch)\n",
    "        splitted_docs = splitter.split_documents(lc_docs)\n",
    "        \n",
    "        target_store.add_documents(splitted_docs)\n",
    "        \n",
    "        docs_embedded += len(partial_batch)\n",
    "        pbar.update(len(partial_batch))\n",
    "\n",
    "    pbar.close()\n",
    "    print(\n",
    "        f\"Completed indexing from '{source_idx}' to '{target_index}' \"\n",
    "        f\"with {docs_embedded} docs processed.\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 6. SCRIPT_SCORE SEARCH (Cosine Similarity) -- EXCLUDING THE VECTOR FIELD\n",
    "# ------------------------------------------------------------------------\n",
    "def script_score_search(index_name: str, query_vector: list[float], top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Perform a script_score cosine similarity query on the field 'vector'.\n",
    "    Returns the raw Elasticsearch response, excluding the 'vector' field.\n",
    "    \"\"\"\n",
    "    body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"match_all\": {}},\n",
    "                \"script\": {\n",
    "                    \"source\": \"cosineSimilarity(params.query_vector, 'vector') + 1.0\",\n",
    "                    \"params\": {\n",
    "                        \"query_vector\": query_vector\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    # We tell Elasticsearch not to return the 'vector' field in the results\n",
    "    return es_client.search(\n",
    "        index=index_name,\n",
    "        body=body,\n",
    "        _source_excludes=[\"vector\"]  # This ensures the 'vector' field is omitted\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 7. EXTRACT FLIGHT INFO FROM HITS\n",
    "# ------------------------------------------------------------------------\n",
    "def extract_context_from_hits(hits):\n",
    "    \"\"\"\n",
    "    Format flight-related fields from each document, parsing them from the\n",
    "    '_source.text' string. Returns a multi-line string with flight info.\n",
    "    \"\"\"\n",
    "    context = \"\"\n",
    "    for hit in hits:\n",
    "        # 'text' holds lines like \"FlightNum: NF156JH\\nDestCountry: IN\\n...\"\n",
    "        text_str = hit[\"_source\"][\"text\"]\n",
    "        \n",
    "        # Parse each line into a dict, e.g. {\"FlightNum\": \"NF156JH\", \"DestCountry\": \"IN\", ...}\n",
    "        lines_dict = {}\n",
    "        for line in text_str.split(\"\\n\"):\n",
    "            if \": \" in line:  # ensure the line is key: value format\n",
    "                key, val = line.split(\": \", 1)\n",
    "                lines_dict[key] = val.strip()\n",
    "        \n",
    "        # Convert certain numeric/string fields\n",
    "        def to_float(s):\n",
    "            try:\n",
    "                return float(s)\n",
    "            except ValueError:\n",
    "                return 0.0\n",
    "\n",
    "        flight_num = lines_dict.get(\"FlightNum\", \"N/A\")\n",
    "        carrier = lines_dict.get(\"Carrier\", \"N/A\")\n",
    "        origin_city = lines_dict.get(\"OriginCityName\", \"N/A\")\n",
    "        origin_country = lines_dict.get(\"OriginCountry\", \"N/A\")\n",
    "        dest_city = lines_dict.get(\"DestCityName\", \"N/A\")\n",
    "        dest_country = lines_dict.get(\"DestCountry\", \"N/A\")\n",
    "        distance_km = to_float(lines_dict.get(\"DistanceKilometers\", \"0\"))\n",
    "        distance_mi = to_float(lines_dict.get(\"DistanceMiles\", \"0\"))\n",
    "        flight_time_hr = to_float(lines_dict.get(\"FlightTimeHour\", \"0\"))\n",
    "        origin_weather = lines_dict.get(\"OriginWeather\", \"N/A\")\n",
    "        dest_weather = lines_dict.get(\"DestWeather\", \"N/A\")\n",
    "        avg_ticket_price = to_float(lines_dict.get(\"AvgTicketPrice\", \"0\"))\n",
    "        timestamp = lines_dict.get(\"timestamp\", \"N/A\")\n",
    "        \n",
    "        # Convert booleans\n",
    "        flight_delay_str = lines_dict.get(\"FlightDelay\", \"False\")\n",
    "        flight_delay = (flight_delay_str.lower() == \"true\")\n",
    "        cancelled_str = lines_dict.get(\"Cancelled\", \"False\")\n",
    "        cancelled = (cancelled_str.lower() == \"true\")\n",
    "\n",
    "        # Format final text block\n",
    "        flight_info = (\n",
    "            f\"Flight Number: {flight_num}\\n\"\n",
    "            f\"Carrier: {carrier}\\n\"\n",
    "            f\"From: {origin_city} ({origin_country})\\n\"\n",
    "            f\"To: {dest_city} ({dest_country})\\n\"\n",
    "            f\"Distance: {distance_km:,.2f} km ({distance_mi:,.2f} miles)\\n\"\n",
    "            f\"Flight Time: {flight_time_hr:.2f} hours\\n\"\n",
    "            f\"Weather at Origin: {origin_weather}\\n\"\n",
    "            f\"Weather at Destination: {dest_weather}\\n\"\n",
    "            f\"Average Ticket Price: ${avg_ticket_price:,.2f}\\n\"\n",
    "            f\"Delay: {'Yes' if flight_delay else 'No'}\\n\"\n",
    "            f\"Cancellation: {'Yes' if cancelled else 'No'}\\n\"\n",
    "            f\"Date: {timestamp}\\n\"\n",
    "            \"----------------------------------------\\n\"\n",
    "        )\n",
    "        context += flight_info\n",
    "\n",
    "    return context\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# 8. COMBINED QUERY FUNCTION\n",
    "# ------------------------------------------------------------------------\n",
    "def query_vector_store(query_text: str, top_k=5):\n",
    "    \"\"\"\n",
    "    1. Embed the query with OllamaEmbeddings.\n",
    "    2. Run a script_score search on the newly created index.\n",
    "    3. Return the hits so we can do something with them (like extract flight info).\n",
    "    \"\"\"\n",
    "    print(f\"\\nQuery: {query_text}\")\n",
    "    query_vec = ollama_embeddings.embed_query(query_text)\n",
    "\n",
    "    response = script_score_search(target_index, query_vec, top_k)\n",
    "    hits = response[\"hits\"][\"hits\"]\n",
    "    return hits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_12081/3279058841.py:130: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  total_docs = es_client.count(index=source_idx)['count']\n",
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_12081/3279058841.py:82: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  for doc in scan:\n",
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_12081/3279058841.py:141: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  break\n",
      "Indexing: 100%|██████████| 200/200 [01:21<00:00,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed indexing from 'kibana_sample_data_flights' to 'flights_with_embeddings_ollama_lc' with 200 docs processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "index_documents_from_source(source_index, es_store, batch_size=50, max_docs=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: show me a cheap flight to mexico from canada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cg/npjcl03j1_g_q4sw465xcyx00000gn/T/ipykernel_12081/3279058841.py:183: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  return es_client.search(\n"
     ]
    }
   ],
   "source": [
    "# 2) Example query\n",
    "query_text = \"show me a cheap flight to mexico from canada\"\n",
    "hits = query_vector_store(query_text, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXTRACTED CONTEXT:\n",
      " Flight Number: FZ1FWP0\n",
      "Carrier: Kibana Airlines\n",
      "From: Mexico City (MX)\n",
      "To: Ottawa (CA)\n",
      "Distance: 3,589.92 km (2,230.67 miles)\n",
      "Flight Time: 3.52 hours\n",
      "Weather at Origin: Rain\n",
      "Weather at Destination: Clear\n",
      "Average Ticket Price: $937.73\n",
      "Delay: No\n",
      "Cancellation: No\n",
      "Date: 2025-01-13T20:42:03\n",
      "----------------------------------------\n",
      "Flight Number: NF156JH\n",
      "Carrier: Kibana Airlines\n",
      "From: Mexico City (MX)\n",
      "To: Hyderabad (IN)\n",
      "Distance: 15,939.01 km (9,904.04 miles)\n",
      "Flight Time: 18.98 hours\n",
      "Weather at Origin: Rain\n",
      "Weather at Destination: Hail\n",
      "Average Ticket Price: $655.36\n",
      "Delay: No\n",
      "Cancellation: No\n",
      "Date: 2025-01-13T05:32:47\n",
      "----------------------------------------\n",
      "Flight Number: U9MODFN\n",
      "Carrier: ES-Air\n",
      "From: Mexico City (MX)\n",
      "To: San Francisco (US)\n",
      "Distance: 3,027.21 km (1,881.02 miles)\n",
      "Flight Time: 2.66 hours\n",
      "Weather at Origin: Hail\n",
      "Weather at Destination: Heavy Fog\n",
      "Average Ticket Price: $928.53\n",
      "Delay: No\n",
      "Cancellation: No\n",
      "Date: 2025-01-13T08:31:31\n",
      "----------------------------------------\n",
      "Flight Number: S6BPNLR\n",
      "Carrier: JetBeats\n",
      "From: Mexico City (MX)\n",
      "To: Manchester (GB)\n",
      "Distance: 8,734.74 km (5,427.52 miles)\n",
      "Flight Time: 9.10 hours\n",
      "Weather at Origin: Cloudy\n",
      "Weather at Destination: Thunder & Lightning\n",
      "Average Ticket Price: $481.71\n",
      "Delay: No\n",
      "Cancellation: Yes\n",
      "Date: 2025-01-13T23:04:05\n",
      "----------------------------------------\n",
      "Flight Number: 58U013N\n",
      "Carrier: Kibana Airlines\n",
      "From: Mexico City (MX)\n",
      "To: Xi'an (CN)\n",
      "Distance: 13,358.24 km (8,300.43 miles)\n",
      "Flight Time: 13.10 hours\n",
      "Weather at Origin: Damaging Wind\n",
      "Weather at Destination: Clear\n",
      "Average Ticket Price: $730.04\n",
      "Delay: No\n",
      "Cancellation: No\n",
      "Date: 2025-01-13T05:13:00\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3) Extract context from hits\n",
    "context_str = extract_context_from_hits(hits)\n",
    "print(\"EXTRACTED CONTEXT:\\n\", context_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
